#! /usr/bin/env python

import netCDF4
import re, os
import traceback
import statvfs
import numpy as np
import shutil
import time
import stat
import datetime
from general_utils import daytimeconv
from himawari8.slotmapping import hm2slot
from himawari8.navigation import proj4_navigation as h08pnav
from general_utils.basic_logger import make_logger



VA_NAME, VA_TYPE, VA_FILL_VALUE,VA_UNITS, VA_LONG_NAME = 'name', 'type', '_FillValue', 'units', 'long_name'





logger = make_logger(__name__)



#TODO DOC

H08_CHANNEL_NUMBERS = ('01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '18', '19', '20', '21')

__H08_CHANNEL_NAMES_TUP = ('VIS047_2000', 'VIS051_2000', 'VIS064_2000', 'VIS086_2000', 'VIS160_2000', 'VIS229_2000', 'IR390_2000', 'IR620_2000', 'IR690_2000', 'IR730_2000', 'IR860_2000', 'IR960_2000', 'IR104_2000', 'IR112_2000', 'IR124_2000', 'IR133_2000', 'VIS047_1000', 'VIS051_1000', 'VIS064_0500', 'VIS086_1000')
__H08_CHANNEL_DIMS_TUP = ((5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (5500, 5500), (11000, 11000), (11000, 11000), (22000, 22000), (11000, 11000))

H08_CHANNEL_NAMES = dict(zip(H08_CHANNEL_NUMBERS,__H08_CHANNEL_NAMES_TUP))

H08_CHANNEL_DIMS = dict(zip(H08_CHANNEL_NUMBERS,__H08_CHANNEL_DIMS_TUP))

__H08_CHANNEL_ABOM_NAMES_TUP = (  'channel_0001_brf', 'channel_0002_brf', 'channel_0003_brf',
                            'channel_0004_brf', 'channel_0005_brf', 'channel_0006_brf',
                            'channel_0007_brightness_temperature', 'channel_0008_brightness_temperature', 'channel_0009_brightness_temperature',
                            'channel_0010_brightness_temperature', 'channel_0011_brightness_temperature', 'channel_0012_brightness_temperature',
                            'channel_0013_brightness_temperature', 'channel_0014_brightness_temperature', 'channel_0015_brightness_temperature',
                            'channel_0016_brightness_temperature', 'channel_0001_brf', 'channel_0002_brf',
                            'channel_0003_brf', 'channel_0004_brf')


H08_CHANNEL_ABOM_NAMES = dict(zip(H08_CHANNEL_NUMBERS,__H08_CHANNEL_ABOM_NAMES_TUP))


CHANNELS_CHUNKSIZES = [4,8,32,32]

__H08_CHANNEL_NCVARS_TUP = (
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS047_2000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 1 at 0.47 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS051_2000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 2 at 0.51 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS064_2000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 3 at 0.64 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS086_2000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 4 at 0.86 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS160_2000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 5 at 1.61 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS229_2000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 6 at 2.26 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR390_2000', VA_LONG_NAME: u'brightness temperature for channel 7 at 3.89 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR620_2000', VA_LONG_NAME: u'brightness temperature for channel 8 at 6.24 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR690_2000', VA_LONG_NAME: u'brightness temperature for channel 9 at 6.94 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR730_2000', VA_LONG_NAME: u'brightness temperature for channel 10 at 7.35 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR860_2000', VA_LONG_NAME: u'brightness temperature for channel 11 at 8.59 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR960_2000', VA_LONG_NAME: u'brightness temperature for channel 12 at 9.64 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR104_2000', VA_LONG_NAME: u'brightness temperature for channel 13 at 10.41 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR112_2000', VA_LONG_NAME: u'brightness temperature for channel 14 at 11.24 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR124_2000', VA_LONG_NAME: u'brightness temperature for channel 15 at 12.38 um'},
                            {VA_UNITS: u'K', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'IR133_2000', VA_LONG_NAME: u'brightness temperature for channel 16 at 13.28 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS047_1000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 1 at 0.47 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS051_1000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 2 at 0.51 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS064_0500', VA_LONG_NAME: u'bidirectional reflectance factor for channel 3 at 0.64 um'},
                            {VA_UNITS: u'1', VA_FILL_VALUE: 1e+20, VA_TYPE: 'f4', VA_NAME: 'VIS086_1000', VA_LONG_NAME: u'bidirectional reflectance factor for channel 4 at 0.86 um'},
                        )


H08_CHANNEL_NCVARS = dict(zip(H08_CHANNEL_NUMBERS, __H08_CHANNEL_NCVARS_TUP))



MAX_CHANEL_NUMBER = 21 # so far


FILENAMECOL='FILENAME'
CHANNELNCOL ='CHANNEL'
DFBCOL='DFB'
SLOTCOL='SLOT'
SIZECOL='SIZE'

NC_FILENAME_LENGTH = 24

ALL_NC_RE_STRING='^IDE002(\d{2}).(\d{12}).nc$'
#NC_FILES_ARRAY_DTYPE =  [(FILENAMECOL, 'S%s' % NC_FILENAME_LENGTH),  (DFBCOL, 'i4'), (SLOTCOL, 'i4'), (SIZECOL, 'i8')]
NC_FILES_ARRAY_DTYPE =  [(FILENAMECOL, 'S%s' % NC_FILENAME_LENGTH), (CHANNELNCOL,'u1'),  (DFBCOL, 'i4'), (SLOTCOL, 'i4'), (SIZECOL, 'i8')]





'''
def get_nc_info1(folder=None, min_nc_age_seconds=None, channel_numbers=None):
    """
    Given a
    :param folder:
    :param min_nc_age_seconds:
    :return:
    """

    if min_nc_age_seconds is None:
        min_nc_age_seconds = 1
    if not folder.endswith('/'):
        folder += '/'
    res = '^IDE002(\d{2}).(\d{12}).nc$'
    rex = re.compile(res)

    nc_gen = ((f, int(f[6:8]),daytimeconv.yyyymmdd2dfb(f[9:17]), hm2slot(h=f[17:19], m=f[19:21]), os.path.getsize(folder+f)) for i, f in enumerate(os.listdir(folder)) if rex.match(f) and file_age_in_seconds(os.path.join(folder,f))> min_nc_age_seconds)

    nc_files = np.fromiter(nc_gen, dtype=NC_FILES_ARRAY_DTYPE)
    mask = np.zeros_like(nc_files, dtype=np.uint8).astype(np.bool)
    if channel_numbers:
        try:
            chnit = iter(channel_numbers)
        except Exception:
            raise ValueError('Channel numbers should be an iterable')

        nc_channel_numbers = np.unique(nc_files[CHANNELNCOL])
        bad_channels = [e for e in channel_numbers if e not in nc_channel_numbers]
        if len(bad_channels) > 0:
            raise ValueError('Channel {} does not exists in folder {}. Available channel are {}'.format(str(bad_channels), folder, str(nc_channel_numbers)))
        for v in chnit:
            mask = nc_files[CHANNELNCOL]== v | mask
    nc_files = nc_files[mask]
    return np.sort(nc_files, order=[DFBCOL, SLOTCOL])

def get_nc_info(folder=None, min_nc_age_seconds=None, channel_numbers=None):
    """
    Given a folder woth H08 NetCDF data from ABOM this function will read the requested files/channels that are older than
    min_nc_age_seconds and create a record sorted array with information about the channels.
    The record array contains folowing fields:
        FILENAME, CHANNEL, DFB, SLOT, SIZE

    :param folder: str, the folder where H08 data resides
    :param min_nc_age_seconds: number, int, the minimum age in seconds of a channel/file/. Only files older than this age are collected
    :param channel_numbers, iter, iterable of ints representing channels to be read. If None than all channels are collected
    :return: numpy record array with 5 fieldsFILENAME, CHANNEL, DFB, SLOT, SIZE
    """

    if min_nc_age_seconds is None:
        min_nc_age_seconds = 1
    if not folder.endswith('/'):
        folder += '/'

    try:
        _ = iter(channel_numbers)
        first_number = np.unique(['{0:02d}'.format(i)[0] for i in channel_numbers])
        second_number = np.unique(['{0:02d}'.format(i)[1] for i in channel_numbers])
        res = '^IDE002([%s][%s]).(\d{12}).nc$' % (','.join(first_number), ','.join(second_number))
    except Exception:
        res = '^IDE002(\d{2}).(\d{12}).nc$'


    rex = re.compile(res)
    nc_gen = ((f, int(f[6:8]), daytimeconv.yyyymmdd2dfb(f[9:17]), hm2slot(h=f[17:19], m=f[19:21]), os.path.getsize(folder + f)) for i, f in enumerate(os.listdir(folder)) if rex.match(f) and file_age_in_seconds(os.path.join(folder, f)) > min_nc_age_seconds)
    nc_files = np.fromiter(nc_gen, dtype=NC_FILES_ARRAY_DTYPE)
    return np.sort(nc_files, order=[DFBCOL, SLOTCOL])
'''

def get_nc_info(folder=None, min_nc_age_seconds=0, channel_numbers=None, ):
    """
    Given a folder woth H08 NetCDF data from ABOM this function will read the requested files/channels that are older than
    min_nc_age_seconds and create a record sorted array with information about the channels.
    The record array contains folowing fields:
        FILENAME, CHANNEL, DFB, SLOT, SIZE

    :param folder: str, the folder where H08 data resides
    :param min_nc_age_seconds: number, int, the minimum age in seconds of a channel/file/. Only files older than this age are collected
    :param channel_numbers, iter, iterable of ints representing channels to be read. If None than all channels are collected
    :return: numpy record array with 5 fieldsFILENAME, CHANNEL, DFB, SLOT, SIZE
    """

    if not folder.endswith('/'):
        folder += '/'
    try:
        len(channel_numbers)
        nc_gen = ((f, int(f[6:8]), daytimeconv.yyyymmdd2dfb(f[9:17]), hm2slot(h=f[17:19], m=f[19:21]), os.path.getsize(folder + f)) for i, f in enumerate(os.listdir(folder)) if int(f[6:8]) in channel_numbers and file_age_in_seconds(os.path.join(folder, f)) > min_nc_age_seconds and f.split('.')[-1] == 'nc')
    except TypeError:
        nc_gen = ((f, int(f[6:8]), daytimeconv.yyyymmdd2dfb(f[9:17]), hm2slot(h=f[17:19], m=f[19:21]), os.path.getsize(folder + f)) for i, f in enumerate(os.listdir(folder)) if file_age_in_seconds(os.path.join(folder, f)) > min_nc_age_seconds and f.split('.')[-1] == 'nc')
    nc_files = np.fromiter(nc_gen, dtype=NC_FILES_ARRAY_DTYPE)
    nc_files.sort(order=[DFBCOL, CHANNELNCOL, SLOTCOL])
    return nc_files

def mkdir_recursive(path):
    """
        make dirs in the path recursively
        :param path:
        :return:
    """

    sub_path = os.path.dirname(path)
    if not os.path.exists(sub_path):
        mkdir_recursive(sub_path)
    if not os.path.exists(path):
        os.mkdir(path)

def is_running(name=None, ipid=os.getpid()):
    """
    Checks is the python  script "name" is running. Basically this functions should be used
    from within the same script
    If the script is running:
        returns True, pid, string representing the running time in format [[dd-]hh:]mm:ss
    else
        If is not running returns False, None, None

    @args:
        @name, str, the name of program|script
        @ipid, int, the rocess id of the
    """

    if not os.path.isabs(name):
        raise ValueError, 'The script name "%s" is not an absolute path' % name
    py_processes = os.popen("ps aux |grep python").read()
    exists, pid, start_time_string  = False, None, None
    for pline in py_processes.splitlines():
        if name in pline:
            pid = int(pline.rsplit()[1])
            if pid != ipid:
                start_time_string = os.popen('ps -p %s -o etime=' % pid).read().strip()
                exists = True
                break
        if exists:
            break

    return exists, pid, start_time_string

def is_writable(folder):
    '''
    The function replaces os.access(path, os.W_OK) because of nfs disk.
    '''
    filename=os.path.join(folder,"temp_test_write")
    try:
        fd=open(filename,'w')
    except:
        return False
    fd.close()
    if not(os.access(filename,os.F_OK)):
        return False
    os.remove(filename)
    return True

def freespace(folder):
    s = os.statvfs(folder)
    return s[statvfs.F_BAVAIL] * long(s[statvfs.F_BSIZE])

def file_age_in_seconds(pathname):
    return time.time() - os.stat(pathname)[stat.ST_MTIME]

def backup_and_move(nc_folder=None, backup_folder=None, to_process_nc_folder=None,files=None,  channels_to_process=None) :

    """
        backup nc files and moves them from nc_folder or incoming folder to the temporary procesisng folder
        or to_process_nc_folder


    """




    failed_files = np.ones(len(files)).astype(np.bool)


    for i, e in enumerate(files):
        fn, chn, dfb, slot, sz = e
        nc_full_path = os.path.join(nc_folder, fn)
        dt = datetime.datetime.strptime(fn[9:21], '%Y%m%d%H%M')
        replaceable_chunk = '%d/%02d/' % (dt.year, dt.month) # placeholder for year and month

        bkp_folder = os.path.join(backup_folder,replaceable_chunk)

        mkdir_recursive(bkp_folder)
        bkp_full_path = os.path.join(bkp_folder, fn)

        preprocessed_nc_full_path = os.path.join(to_process_nc_folder, fn)

        #make backup
        try:
            logger.debug('Backing-up and moving NC  {} to {}'.format(fn, bkp_full_path) )

            #copy or move to backup
            if chn in channels_to_process:
                logger.info('Channel %s for file %s will be copied to backup ' % (chn, fn))
                shutil.copy(nc_full_path, bkp_full_path)
                shutil.move(nc_full_path, preprocessed_nc_full_path)

            else:
                logger.info('Channel %s for file %s will be moved to backup ' % (chn, fn))
                shutil.move(nc_full_path, bkp_full_path)

        except Exception as e:
            logger.error('Error %s occured while backing-up or moving file %s' % (traceback.print_exc(e), nc_full_path))
            failed_files[i] = False

    return failed_files

def move(nc_folder=None, to_process_nc_folder=None,files=None,  channels_to_process=None) :

    """
        moves nc files a from nc_folder or incoming folder to the temporary processing folder
        or to_process_nc_folder


    """




    failed_files = np.ones(len(files)).astype(np.bool)


    for i, e in enumerate(files):
        fn, chn, dfb, slot, sz = e
        nc_full_path = os.path.join(nc_folder, fn)
        dt = datetime.datetime.strptime(fn[9:21], '%Y%m%d%H%M')
        replaceable_chunk = '%d/%02d/' % (dt.year, dt.month) # placeholder for year and month



        preprocessed_nc_full_path = os.path.join(to_process_nc_folder, fn)

        #make backup
        try:
            logger.debug('Moving NC  {} to {}'.format(fn, to_process_nc_folder) )

            #copy or move to backup
            if chn in channels_to_process:
                logger.debug('Channel %s for file %s will be copied to backup ' % (chn, fn))

                shutil.move(nc_full_path, preprocessed_nc_full_path)


        except Exception as e:
            logger.error('Error %s occured while backing-up or moving file %s' % (traceback.print_exc(e), nc_full_path))
            failed_files[i] = False

    return failed_files

def clean_processed_nc(processed_nc_folder=None, max_nc_age_days=None):
    """

    """
    logger.info('CLEANING %s' % processed_nc_folder)
    fls = get_nc_info(folder=processed_nc_folder)
    for fn in fls[FILENAMECOL]:
        nc_full_path = os.path.join(processed_nc_folder, fn)
        nc_age = datetime.datetime.now()-datetime.datetime.fromtimestamp(os.path.getmtime(nc_full_path))
        if nc_age.days > max_nc_age_days:

            # remove
            logger.debug('REMOVING %s ' % nc_full_path )

            os.remove(nc_full_path)



'''
def latlon_read_dfb_slot_lat_lon_nc(ncfile, chan, Dfbs, Slots, bbox, interpolate='nearest'):
    if (interpolate not in ['nearest', 'bilinear']):
        logger.error("unsupported interpolation type: %s",interpolate)
        return None

    if (bbox is None):
        logger.error("no or None bbox to read : %s",ncfile)
        return None

    dfbs=Dfbs[1]-Dfbs[0]+1
    slots=Slots[1]-Slots[0]+1
    latlon_rows=bbox.height
    latlon_cols=bbox.width
    output_data = numpy.zeros((dfbs,slots,latlon_rows,latlon_cols), dtype=numpy.float32)
    output_data[:,:,:,:] = numpy.nan


    #open NC for reading
    try:
        rootgrpout = netCDF4.Dataset(ncfile, 'r')
    except:
        logger.error('Unable to open file %s. %s Skipping' % (ncfile, str(sys.exc_info())))
        return None
    dimDictout=rootgrpout.dimensions
    varDictout=rootgrpout.variables
    if 'dfb' in varDictout.keys():
        dfb_var_name = 'dfb'
    elif 'month' in varDictout.keys():
        dfb_var_name = 'month'
    elif 'day' in varDictout.keys():
        dfb_var_name = 'day'
    else:
        logger.error('Variable dfb not found in nc %s . Skipping' % (ncfile))
        rootgrpout.close()
        return None
    dfb_var_out=varDictout[dfb_var_name]

    if 'slot' in varDictout.keys():
        slot_var_name = 'slot'
    elif 'time' in varDictout.keys():
        slot_var_name = 'time'
    elif 'hour' in varDictout.keys():
        slot_var_name = 'hour'
    else:
        logger.error('No variable for slot found in nc %s . Skipping' % (ncfile))
        rootgrpout.close()
        return None
    slot_var_out=varDictout[slot_var_name]

    if chan not in varDictout.keys():
        logger.error('No variable for %s found in nc %s . Skipping' % (chan, ncfile))
        rootgrpout.close()
        return None
    data_var_out=varDictout[chan]

    if 'col' in dimDictout.keys():
        col_name='col'
        row_name='row'
    elif 'longitude' in dimDictout.keys():
        col_name='longitude'
        row_name='latitude'
    elif 'img_x_coordinate' in dimDictout.keys():
        col_name='img_x_coordinate'
        row_name='img_y_coordinate'
    else:
        logger.error('No longitude/latitude or col/row dimensions found in nc %s . Skipping' % (ncfile))
        rootgrpout.close()
        return None
    ncvar_col=varDictout[col_name]
    ncvar_row=varDictout[row_name]
    cols=len(dimDictout[col_name])
    rows=len(dimDictout[row_name])

    #recalculate in and out dfb indexes
    nc_dfb_min, nc_dfb_max = dfb_var_out.valid_range
    dfb_min_out=max(nc_dfb_min, Dfbs[0])
    dfb_max_out=min(nc_dfb_max, Dfbs[1])
    data_dfb_min_idx=dfb_min_out-Dfbs[0]
    data_dfb_max_idx=data_dfb_min_idx+dfb_max_out-dfb_min_out
    nc_dfb_min_idx=dfb_min_out-nc_dfb_min
    nc_dfb_max_idx=nc_dfb_min_idx+dfb_max_out-dfb_min_out


    #recalculate in and out slot indexes
    nc_slot_min, nc_slot_max = slot_var_out.valid_range
    slot_min_out=max(nc_slot_min, Slots[0])
    slot_max_out=min(nc_slot_max, Slots[1])
    data_slot_min_idx=slot_min_out-Slots[0]
    data_slot_max_idx=data_slot_min_idx+slot_max_out-slot_min_out
    nc_slot_min_idx=slot_min_out-nc_slot_min
    nc_slot_max_idx=nc_slot_min_idx+slot_max_out-slot_min_out


    col_min, col_max = ncvar_col.valid_range
    row_min, row_max = ncvar_row.valid_range

    colres=(col_max-col_min)/(cols)
    rowres=(row_max-row_min)/(rows)

    col_min=59.69542
    col_max=221.6084
    row_min=-79.6234
    row_max=79.26367
    colres=rowres=0.02
    cols=rows=5500
    nc_ubbox = latlon.uneven_bounding_box(xmin=col_min, xmax=col_max, ymin=row_min, ymax=row_max, width=cols, height=rows, xresolution=colres, yresolution=rowres)
    seg_ubbox = bbox.get_uneven_bbox()

    if nc_ubbox.equal_resolution(seg_ubbox) and nc_ubbox.contains(seg_ubbox):
        #equal resolution of bboxes - use simplified reader without interpolation
        px_xmin, px_xmax, px_ymin, px_ymax = nc_ubbox.pixel_coords_of_bbox(seg_ubbox)
#        print px_xmin, px_xmax, px_ymin, px_ymax

        try:
            res=data_var_out[nc_dfb_min_idx:nc_dfb_max_idx+1,nc_slot_min_idx:nc_slot_max_idx+1, px_ymin:px_ymax+1,px_xmin:px_xmax+1]
        except:
            logger.error("unable to read from NetCDF file: %s",ncfile)
            print sys.exc_info()
            print 'try to read coordinates:', nc_dfb_min_idx,nc_dfb_max_idx+1,nc_slot_min_idx,nc_slot_max_idx+1, px_ymin,px_ymax+1,px_xmin,px_xmax+1
            rootgrpout.close()
            return None
        output_data[data_dfb_min_idx:data_dfb_max_idx+1,data_slot_min_idx:data_slot_max_idx+1,:,:] = res

    else:
        #no equal resolution of bboxes or not full overlap - use extended reader with interpolation
#        if not nc_ubbox.contains(seg_ubbox) and False:
#            logger.error("unable to read from NetCDF file: %s",ncfile)
#            px_xmin, px_xmax, px_ymin, px_ymax = nc_ubbox.pixel_coords_of_bbox(seg_ubbox)
#            print 'coordinates out of nc bbox:', nc_dfb_min_idx,nc_dfb_max_idx+1,nc_slot_min_idx,nc_slot_max_idx+1, px_ymin,px_ymax+1,px_xmin,px_xmax+1
#            rootgrpout.close()
#            return None

        col_idxs1, row_idxs1, col_idxs2, row_idxs2, col_wghts, row_wghts = seg_ubbox.px_idx_grid_of_second_bbox(nc_ubbox)

#        print col_idxs1, row_idxs1, col_idxs2, row_idxs2, col_wghts, row_wghts, interpolate
        if interpolate=='nearest':
            wh = (col_idxs1 == col_idxs1) & (row_idxs1==row_idxs1) & (col_idxs1 != -9) & (row_idxs1 != -9)
            col_min=col_idxs1[wh].min()
            col_max=col_idxs1[wh].max()
            row_min=row_idxs1[wh].min()
            row_max=row_idxs1[wh].max()
#            print  col_min,col_max,row_min,row_max
#            print nc_dfb_min_idx,  nc_dfb_max_idx, nc_slot_min_idx, nc_slot_max_idx
            try:
                data=data_var_out[nc_dfb_min_idx:nc_dfb_max_idx+1,nc_slot_min_idx:nc_slot_max_idx+1, row_min:row_max+1,col_min:col_max+1]
            except:
                logger.error("unable to read from NetCDF file: %s",ncfile)
                print sys.exc_info()
                rootgrpout.close()
                return None
            res=data[:,:,row_idxs1[wh]-row_min,col_idxs1[wh]-col_min]
        else:
            wh = (col_idxs1 != -9) & (col_idxs2 != -9) & (row_idxs1 != -9) & (row_idxs2 != -9)
            wh = wh & (col_idxs1 == col_idxs1) & (col_idxs2 == col_idxs2) & (row_idxs1==row_idxs1) & (row_idxs2==row_idxs2)
            col_min=min(col_idxs1[wh].min(), col_idxs2[wh].min())
            col_max=max(col_idxs1[wh].max(), col_idxs2[wh].max())
            row_min=min(row_idxs1[wh].min(), row_idxs2[wh].min())
            row_max=max(row_idxs1[wh].max(), row_idxs2[wh].max())

            try:
                data=data_var_out[nc_dfb_min_idx:nc_dfb_max_idx+1,nc_slot_min_idx:nc_slot_max_idx+1,row_min:row_max+1,col_min:col_max+1]
            except:
                logger.error("unable to read from NetCDF file: %s",ncfile)
                print sys.exc_info()
                rootgrpout.close()
                return None

            out_data11=data[:,:,row_idxs1[wh]-row_min,col_idxs1[wh]-col_min]
            out_data12=data[:,:,row_idxs1[wh]-row_min,col_idxs2[wh]-col_min]
            out_data21=data[:,:,row_idxs2[wh]-row_min,col_idxs1[wh]-col_min]
            out_data22=data[:,:,row_idxs2[wh]-row_min,col_idxs2[wh]-col_min]

            aux1 = out_data11 + row_wghts[wh]*(out_data21 - out_data11)
            aux2 = out_data12 + row_wghts[wh]*(out_data22 - out_data12)
            res=aux1 + col_wghts[wh]*(aux2 - aux1)

        output_data[data_dfb_min_idx:data_dfb_max_idx+1,data_slot_min_idx:data_slot_max_idx+1,wh] = res

    rootgrpout.close()
    return output_data
'''


def read_himawari8_nc(ncfile_path=None, ):
    """
    Reads metadata related to projection and data  for an ABOM H08 NetCDF file
    :param ncfile_path: str, full path to the ABOM NetCDF file
    :return: tuple (geotransform, sub-satellite point, proj4 string,  data - [2D numpy array])
    """
    with netCDF4.Dataset(ncfile_path, 'r') as ncd:
        #parse filename e.g. IDE00201.201510061610.nc

        _folder, fname = os.path.split(ncfile_path)
        channel_number_str = fname[6:8]
        abom_channel_name = H08_CHANNEL_ABOM_NAMES[channel_number_str]
        channel_var_def = H08_CHANNEL_NCVARS[channel_number_str]

        var_names = ncd.variables.keys()
        #check if the var name exists
        if abom_channel_name not in var_names:
            raise Exception('{0} variable does not exists in {1}'.format(abom_channel_name,ncfile_path))

        data_var = ncd.variables[abom_channel_name]

        geos_var_name = data_var.grid_mapping

        if not geos_var_name in var_names:
            raise RuntimeError('NetCDF file {} is not a ABOM CF1.6 NetCDF'.format(ncfile_path) )
        geos_var = ncd.variables[geos_var_name]
        gt = tuple(geos_var.GeoTransform)

        ssp = geos_var.longitude_of_projection_origin
        proj4_str = geos_var.proj4

        return gt, ssp,proj4_str,channel_var_def,np.squeeze(data_var[:])

def compute_scan_time_offset(image_line=None, channel_name=None):

    """
    The scan time for a pixel is computed using linear interpolation given that
    the time offset from the nonimal time of images is known for the first and last pixel.
    BOM nc images come with a variable called "scan_line_time". For a 100% accurate comutation one should use these variable.
    However, for an approximate computation the average of the scan times for first and last pixel for all slots in 5 days could suffice
    Example of values for all slots and channel for 5 days at the beginning of Feb 2016
    channels = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '18', '19', '20', '21']
    pat = '/mnt/bay_hdd/HIMAWARI8/2016/02/IDE002{0}.2016020[1-4]*'.format(chn)
    #channel, first pixel offset, last pixel offset

        1: (21., 581.),
        2: (21., 581.),
        3: (21., 547.),
        4: (21.,  580.),
        5: (22.,  581.),
        6: (20.,  580.),
        7: (21.,  580.),
        8: (21.,  581.),
        9: (22.,  581.),
        10: (21.,  580.),
        11: (20.,  580.),
        12: (20.,  580.),
        13: (22.,  581.),
        14: (21.,  581.),
        15: (21.,  580.),
        16: (21. , 580.),
        18: (21.,  581.),
        19: (21.,  581.),
        20: (21.,  580.),
        21: (21.,  580.),


    :param image_line:, int, the line number (0, nl-1)
    :param channel_number: the channel number, int
    :param numer_of_lines: the number of line sin the image

    :return: the scan time offset in seconds for each line number
    """
    d = {
        1: (21., 581.),
        2: (21., 581.),
        3: (21., 547.),
        4: (21.,  580.),
        5: (22.,  581.),
        6: (20.,  580.),
        7: (21.,  580.),
        8: (21.,  581.),
        9: (22.,  581.),
        10: (21.,  580.),
        11: (20.,  580.),
        12: (20.,  580.),
        13: (22.,  581.),
        14: (21.,  581.),
        15: (21.,  580.),
        16: (21. , 580.),
        18: (21.,  581.),
        19: (21.,  581.),
        20: (21.,  580.),
        21: (21.,  580.),
    }
    try:
        chm_number_str = [k for k, v in H08_CHANNEL_NAMES.items() if v == channel_name].pop()
    except IndexError as ie:
        logger.error('{0} is not a valid H08 channel name. Valid channel names are {1}'.format(channel_name, H08_CHANNEL_NAMES.values()))
    nlines, ncols = H08_CHANNEL_DIMS[chm_number_str]
    chn_int = int(chm_number_str)
    assert chn_int in d.keys(), 'Invalid channel number {0}. valid values are {1}'.format(chn_int, d.keys())
    tmin, tmax = d[chn_int]
    return tmin + (tmax-tmin)*image_line/float(nlines)


def compute_scan_time_offset2(bbox=None,  channel_name=None):
    """
    Given a bounding box object and a channel compute the scan time offset for each pixel in the bounding box.

    :param bbox: bbox object
    :param channel_number: int
    :return: 2D array representing the scan time offset in seconds for wach pixel in respect to image nominal time
    """

    d = {
        1: (21., 581.),
        2: (21., 581.),
        3: (21., 547.),
        4: (21.,  580.),
        5: (22.,  581.),
        6: (20.,  580.),
        7: (21.,  580.),
        8: (21.,  581.),
        9: (22.,  581.),
        10: (21.,  580.),
        11: (20.,  580.),
        12: (20.,  580.),
        13: (22.,  581.),
        14: (21.,  581.),
        15: (21.,  580.),
        16: (21. , 580.),
        18: (21.,  581.),
        19: (21.,  581.),
        20: (21.,  580.),
        21: (21.,  580.),
    }
    try:
        chm_number_str = [k for k, v in H08_CHANNEL_NAMES.items() if v == channel_name].pop()
    except IndexError as ie:
        logger.error('{0} is not a valid H08 channel name. Valid channel names are {1}'.format(channel_name,H08_CHANNEL_NAMES.values()))

    chn_int = int(chm_number_str)
    assert chn_int in d.keys(), 'Invalid channel number {0}. valid values are {1}'.format(chn_int, d.keys())
    tmin, tmax = d[chn_int]
    nlines, ncols = H08_CHANNEL_DIMS[chm_number_str]
    lats = bbox.latitudes(array2d=True).astype(np.float32)
    lons = bbox.longitudes(array2d=True).astype(np.float32)
    #actually ssp and geotransform are stored in processed tabel in the database so theoretically they should be retrived from he database
    #however the ssp and geotransform are not variable in time but are variable between channels


    l, c = h08pnav.ll2lc(lat=lats , lon=lons, ssp=140.7, gt=(-5500000.0, 2000.0, 0.0, 5500000.0, 0.0, -2000.0))

    return tmin + (tmax-tmin)*l/float(nlines)



def timeit(method):

    def timed(*args, **kw):
        ts = time.time()
        result = method(*args, **kw)
        te = time.time()

        print '%r (%r, %r) %2.2f sec' % (method.__name__, args, kw, te-ts)
        return result

    return timed

