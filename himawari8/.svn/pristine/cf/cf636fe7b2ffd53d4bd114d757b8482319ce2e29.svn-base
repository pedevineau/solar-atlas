
import os
import shutil
import logging
from general_utils import basic_mail, daytimeconv
from himawari8.slotmapping import HIMAWARI_SLOTS
import traceback
from himawari8.utils import get_nc_info, read_himawari8_nc, CHANNELNCOL, DFBCOL, SLOTCOL, FILENAMECOL, H08_CHANNEL_DIMS, H08_CHANNEL_NCVARS, H08_CHANNEL_NAMES
from nclib.core.const_defs import ST_TIME_MONTH
from himawari8.converters.nc2nc import h08chn2geosnc, read_himawari8_geos_archive
from himawari8.utils import backup_and_move, clean_processed_nc, move
from himawari8.data_classes import  get_table
import peewee

import datetime
import numpy as np



logger = logging.getLogger()
logging.basicConfig()


def h082geos_operational(out_folder=None, incoming_folder=None,backup_folder=None,corrupted_folder=None,to_process_folder=None, processed_folder=None, conn_dict=None, channels_to_backup=None, channels_to_process=None, nc_chunksizes=None, email_receivers=None, min_nc_age_secs=None, nc_prefix=None, nc_time_segtype=None, max_nc_age_days=None, log_level=None, disk_label=None):
    hostname = os.uname()[1]

    #set logger
    logger.setLevel(log_level)

    if backup_folder is not None and channels_to_backup is not None:

        # back-up, check for name copies and move the NC files to preprocessed

        to_backup_nc_files = get_nc_info(folder=incoming_folder, min_nc_age_seconds=min_nc_age_secs,
                                         channel_numbers=channels_to_backup)
        if to_backup_nc_files.size > 0:
            logger.info('Backing-up NC from %s to %s' % (incoming_folder, backup_folder))
            backup_mask = backup_and_move(nc_folder=incoming_folder, backup_folder=backup_folder,
                                          to_process_nc_folder=to_process_folder, files=to_backup_nc_files,
                                          channels_to_process=channels_to_process)

            if backup_mask[backup_mask == False].size > 0:
                basic_mail.mail_process_message_ssl(sender_from='Himawari8 h082geos on {0}'.format(hostname),
                                                    reciever_to=email_receivers,
                                                    subject='Himawari operational processing error',
                                                    message='For unknown reason could not back-up/move %s. Starting the processing but issues could arise' % str(
                                                        to_backup_nc_files[backup_mask == False]))
    else:
        # move the NC files to preprocessed

        to_move_nc_files = get_nc_info(folder=incoming_folder, min_nc_age_seconds=min_nc_age_secs,
                                       channel_numbers=channels_to_process)
        if to_move_nc_files.size > 0:
            logger.info('Moving NC from %s to %s' % (incoming_folder, to_process_nc_folder))
            moved_mask = move(nc_folder=incoming_folder, to_process_nc_folder=to_process_folder, files=to_move_nc_files,
                              channels_to_process=channels_to_process)

            if moved_mask[moved_mask == False].size > 0:
                basic_mail.mail_process_message_ssl(sender_from='Himawari8 h082geos on {0}'.format(hostname),
                                                    reciever_to=email_receivers,
                                                    subject='Himawari operational processing error',
                                                    message='For unknown reason could not move %s. Starting the processing but issues could arise' % str(
                                                        to_move_nc_files[moved_mask == False]))

    nc_files = get_nc_info(folder=to_process_folder)

    #setup DB stuff
    table_name = 'processed'
    processed_table = get_table(table_name=table_name, **conn_dict)
    db_data = [] #this list will hold the records
    if nc_files.size > 0:

        logger.info('Starting to process %s NetCDF files from %s ' % (nc_files.size, to_process_folder ))
        for i, r in enumerate(nc_files):
            try:
                fn, chn, dfb, slot, sz = r
                dt = datetime.datetime.strptime(fn[9:21], '%Y%m%d%H%M')
                to_process_ncfile = os.path.join(to_process_folder, fn)
                logger.debug('CONVERTING ABOM NetCDF %s TO GEOSTATIONARY NetCDF...' % to_process_ncfile)

                gt, ssp, prj_str, channel_var_def, channel_data = read_himawari8_nc(ncfile_path=to_process_ncfile)
                channel_var_def['chunksizes']=nc_chunksizes
                h08chn2geosnc(nc_out_folder=out_folder, channel_array=channel_data, dfb=dfb, slot=slot, prefix=nc_prefix, time_seg_type=nc_time_segtype, proj4_str=prj_str, var_def=channel_var_def, geo_transform=gt, ssp=ssp)
                processed_nc_file = os.path.join(processed_folder, fn)
                shutil.move(to_process_ncfile, processed_nc_file)
                db_data.append({'file_name': fn, 'dfb': dfb, 'slot': slot, 'datetime': dt, 'file_size': sz, 'disk': disk_label, 'channel_name': channel_var_def['name'], 'ssp': ssp, 'gt': str(gt)})
                # push the data to database, assuming nothing weird is happening with DNS and the db server is reachable

                if i % 100 == 0 or i == nc_files.size - 1: #every 100 files or so
                    processed_table.bulk_upsert(data=db_data)
                    db_data = []
            except KeyboardInterrupt as ke:
                logger.debug('The script was interrupted')
                raise ke

            except Exception as e:
                logger.error('%s occurred while converting original NC %s to NetCDF. The file will be moved to %s' % (e, to_process_ncfile, corrupted_folder))
                try:
                    corrupted_nc_path = os.path.abspath(os.path.join(corrupted_folder, fn))
                    shutil.move(to_process_ncfile,corrupted_nc_path)
                except OSError as ose:
                    logger.error('%s occurred while moving %s to %s. ' % (str(ose), to_process_ncfile, corrupted_folder))


            finally:
                #push the data to db
                if 'db_data' in locals():
                    if len(db_data) > 0:
                        processed_table.bulk_upsert(data=db_data)

        clean_processed_nc(processed_nc_folder=processed_folder, max_nc_age_days=max_nc_age_days)
        return nc_files
    else:
        logger.info('No NetCDF files were found in {0} or {1}'.format(incoming_folder, to_process_folder))


def h082geos_operational_multislot(out_folder=None, incoming_folder=None, backup_folder=None, corrupted_folder=None,
                         to_process_folder=None, processed_folder=None, conn_dict=None, channels_to_backup=None,
                         channels_to_process=None, nc_chunksizes=None, email_receivers=None, min_nc_age_secs=None,
                         nc_prefix=None, nc_time_segtype=None, max_nc_age_days=None, log_level=None, disk_label=None):
    start = datetime.datetime.now()
    hostname = os.uname()[1]

    # set logger
    logger.setLevel(log_level)


    if backup_folder is not None and channels_to_backup is not None:

        # back-up, check for name copies and move the NC files to preprocessed

        to_backup_nc_files = get_nc_info(folder=incoming_folder, min_nc_age_seconds=min_nc_age_secs,channel_numbers=channels_to_backup)
        if to_backup_nc_files.size > 0:
            logger.info('Backing-up NC from %s to %s' % (incoming_folder, backup_folder))
            backup_mask = backup_and_move(nc_folder=incoming_folder, backup_folder=backup_folder,
                                          to_process_nc_folder=to_process_folder, files=to_backup_nc_files,
                                          channels_to_process=channels_to_process)

            if backup_mask[backup_mask == False].size > 0:
                basic_mail.mail_process_message_ssl(sender_from='Himawari8 h082geos on {0}'.format(hostname),
                                                    reciever_to=email_receivers,
                                                    subject='Himawari operational processing error',
                                                    message='For unknown reason could not back-up/move %s. Starting the processing but issues could arise' % str(
                                                        to_backup_nc_files[backup_mask == False]))
    else:
        # back-up, check for name copies and move the NC files to preprocessed

        to_move_nc_files = get_nc_info(folder=incoming_folder, min_nc_age_seconds=min_nc_age_secs, channel_numbers=channels_to_process)
        if to_move_nc_files.size > 0:
            logger.info('Moving NC from %s to %s' % (incoming_folder, to_process_folder))
            moved_mask = move(nc_folder=incoming_folder,to_process_nc_folder=to_process_folder, files=to_move_nc_files, channels_to_process=channels_to_process)

            if moved_mask[moved_mask == False].size > 0:
                basic_mail.mail_process_message_ssl(sender_from='Himawari8 h082geos on {0}'.format(hostname),
                                                    reciever_to=email_receivers,
                                                    subject='Himawari operational processing error',
                                                    message='For unknown reason could not move %s. Starting the processing but issues could arise' % str(to_move_nc_files[moved_mask == False]))

    nc_files = get_nc_info(folder=to_process_folder)

    # setup DB stuff
    table_name = 'processed'
    processed_table = get_table(table_name=table_name, **conn_dict)

    db_data = []  # this list will hold the records
    data_array = np.empty(())
    if nc_files.size > 0:

        logger.info('Starting to process %s NetCDF files from %s ' % (nc_files.size, to_process_folder))
        slot_step = nc_chunksizes[1] # the step size equals the chunk_size in slot dimension, in order to minimize compression in the netCDF lib

        nslots = len(HIMAWARI_SLOTS)
        slot_step = nc_chunksizes[1]
        interval_mask = np.zeros(slot_step).astype(np.bool)
        n_intervals = nslots//slot_step
        dfbs = np.unique(nc_files[DFBCOL])

        for dfb in dfbs:
            logger.debug('PROCESSING DFB|DATE {0}|{1}'.format(dfb, daytimeconv.dfb2date(dfb)))
            dfbm = nc_files[DFBCOL] == dfb
            dfb_ncfiles = nc_files[dfbm]
            _date = daytimeconv.dfb2date(dfb)
            channels = np.unique(dfb_ncfiles[CHANNELNCOL])

            for channel in channels:
                logger.info('PROCESSING DATE {0} CHANNEL {1}'.format(dfb, channel))
                chn_str = '{:02d}'.format(channel)

                data_var_name = H08_CHANNEL_NAMES[chn_str]
                channel_nl, channel_nc = H08_CHANNEL_DIMS[chn_str]
                channel_dtype = H08_CHANNEL_NCVARS[chn_str]['type']
                channel_fill_value = H08_CHANNEL_NCVARS[chn_str]['_FillValue']
                out_nc_name = '{0}_{1}__{2}_{3}_{4:02d}.nc'.format(nc_prefix, data_var_name,nc_time_segtype.name,_date.year, _date.month)
                out_nc_path = os.path.join(out_folder, out_nc_name)
                #logger.info(out_nc_path)
                #fix the bug bug checking of the file exists
                if os.path.exists(out_nc_path):
                    reread_data = True
                else:
                    reread_data = False
                    data_array = np.empty((slot_step, channel_nl, channel_nc), dtype=channel_dtype)
                    data_array[:] = channel_fill_value
                    data_array[:] = channel_fill_value
                dfb_chn_ncfiles = dfb_ncfiles[dfb_ncfiles[CHANNELNCOL]==channel]
                for i in range(n_intervals):
                    interval_slot_start = (i*slot_step)+1
                    interval_slot_end = (i+1)*slot_step
                    slot_interval_mask = (dfb_chn_ncfiles[SLOTCOL] >= interval_slot_start) &  (dfb_chn_ncfiles[SLOTCOL] <= interval_slot_end)
                    dfb_chn_slot_step_ncfiles = dfb_chn_ncfiles[slot_interval_mask]

                    if dfb_chn_slot_step_ncfiles.size > 0:
                        slts = dfb_chn_slot_step_ncfiles[SLOTCOL]
                        if reread_data:
                            read_slots = [i for i in range(interval_slot_start, interval_slot_end+1)]
                            data_array = read_himawari8_geos_archive(ncfile=out_nc_path,dfb=dfb, slots=read_slots)
                        logger.debug('PROCESSING {0} ABOM NC FILES FOR DATE {1} CHANNEL {2} SLOT(S) {3}'.format(dfb_chn_slot_step_ncfiles.size,  _date, channel,  slts ))
                        try:
                            for r  in  dfb_chn_slot_step_ncfiles:
                                _fn, _chn, _dfb, _slot, _sz = r
                                ri = _slot - interval_slot_start
                                to_process_ncfile = os.path.join(to_process_folder, _fn)
                                gt, ssp, prj_str, channel_var_def, channel_data = read_himawari8_nc(ncfile_path=to_process_ncfile)
                                channel_var_def['chunksizes'] = nc_chunksizes
                                data_array[ri,:] = channel_data
                            #logger.debug(slts)

                            h08chn2geosnc(nc_out_folder=out_folder,
                                          channel_array=data_array,
                                          dfb=dfb,
                                          slot=(interval_slot_start, interval_slot_end),
                                          prefix=nc_prefix,
                                          time_seg_type=nc_time_segtype,
                                          proj4_str=prj_str,
                                          var_def=channel_var_def,
                                          geo_transform=gt, ssp=ssp)

                            data_array[:] = channel_fill_value

                            for r in dfb_chn_slot_step_ncfiles:
                                _fn, _chn, _dfb, _slot, _sz = r
                                to_process_ncfile = os.path.join(to_process_folder, _fn)
                                processed_nc_file = os.path.join(processed_folder, _fn)
                                logger.debug('MOVING {0} to {1}'.format(to_process_ncfile, processed_nc_file))
                                dt = datetime.datetime.strptime(_fn[9:21], '%Y%m%d%H%M')
                                db_data.append({'file_name': _fn, 'dfb': _dfb, 'slot': _slot, 'datetime': dt, 'file_size': _sz,'disk': disk_label, 'channel_name': channel_var_def['name'], 'ssp': ssp,'gt': str(gt)})
                                shutil.move(to_process_ncfile, processed_nc_file)
                            processed_table.bulk_upsert(data=db_data)
                            db_data = []

                        except KeyboardInterrupt as ke:
                            logger.debug('The script was interrupted')

                            raise ke

                        except peewee.PeeweeException as pe:
                            logger.error('Database error. {0}'.format(traceback.print_exc(pe)))

                        except Exception as e:
                            logger.error(
                                '%s occurred while converting NC files %s to NetCDF. The files will be moved to %s' % (traceback.print_exc(e), dfb_chn_slot_step_ncfiles[FILENAMECOL], corrupted_folder))
                            try:
                                for r in dfb_chn_slot_step_ncfiles:
                                    _fname = r[0]
                                    to_process_ncfile = os.path.join(os.path.join(to_process_folder, _fname))
                                    corrupted_nc_file = os.path.abspath(os.path.join(corrupted_folder, _fname))
                                    shutil.move(to_process_ncfile, corrupted_nc_file)
                            except OSError as ose:
                                logger.error('%s occurred while moving %s to %s. ' % (str(ose), dfb_chn_slot_step_ncfiles[FILENAMECOL], corrupted_folder))
                        finally:

                            if 'db_data' in locals():
                                if len(db_data) > 0:
                                    processed_table.bulk_upsert(data=db_data)
                                    db_data = []
        end = datetime.datetime.now()
        logger.info('CTIME {0} processed in {1} '.format(len(nc_files), end-start))
        clean_processed_nc(processed_nc_folder=processed_folder, max_nc_age_days=max_nc_age_days)

        return nc_files
    else:
        logger.info('No NetCDF files were found in {0} or {1}'.format(incoming_folder, to_process_folder))


if __name__ == '__main__':
    ##################################################
    # IO FOLDERS
    ###################################################

    incoming_nc_folder  = '/home/jano/Desktop/himawari/OPERATIONAL/INCOMING/'
    backup_nc_folder       = '/home/jano/Desktop/himawari/OPERATIONAL/BACKUP'
    corrupted_nc_folder = '/home/jano/Desktop/himawari/OPERATIONAL/CORRUPTED/'
    to_process_nc_folder = '/home/jano/Desktop/himawari/OPERATIONAL/TO_PROCESS/'
    processed_nc_folder = '/home/jano/Desktop/himawari/OPERATIONAL/PROCESSED/'
    geos_nc_folder = '/home/jano/Desktop/himawari/OPERATIONAL/OUTNC/GEOS/'


    ##################################################
    # GENERAL PROCESSING
    ###################################################
    loglevel = 'DEBUG'
    #currently we receive 20 channels where teh first four are the downsampled version of the last four
    channels_to_backup = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21]
    channels_to_process = [3,7,8,13,15]
    disk_label=None

    ##################################################
    # COMPUTATINAL PROCESSING
    ###################################################

    min_nc_age_secs = 1

    ##################################################
    # NC PROPERTIES
    ###################################################
    nc_time_segtype = ST_TIME_MONTH
    max_nc_age_days = 60
    nc_prefix = 'H08GEOS'
    nc_chunksizes = [4,8,32,32]



    ##################################################
    # DATABASE
    ###################################################
    dbengine = 'postgresql'
    host = 'dbsatarchive'
    database = 'himawari_archive'
    password = 'itNov6'
    user = 'sat_oper'
    port = 5432
    conn_dict = {'dbengine': dbengine, 'host': host, 'port': port, 'database': database, 'user': user, 'password': password}



    ##################################################
    # EMAIL
    ###################################################

    # email_receivers = ['tomas.cebecauer@geomodel.eu', 'ioan.ferencik@geomodel.eu']
    email_receivers = ['ioan.ferencik@geomodel.eu']



    h082geos_operational(out_folder=geos_nc_folder,
            incoming_folder=incoming_nc_folder,
            backup_folder=backup_nc_folder,
            corrupted_folder=corrupted_nc_folder,
            to_process_folder=to_process_nc_folder,
            processed_folder=processed_nc_folder,
            conn_dict=conn_dict,
            channels_to_backup=channels_to_backup,
            channels_to_process=channels_to_process,
            nc_chunksizes=nc_chunksizes,
            email_receivers=email_receivers,
            min_nc_age_secs=min_nc_age_secs,
            nc_time_segtype=nc_time_segtype,
            nc_prefix=nc_prefix,
            max_nc_age_days=max_nc_age_days,
            log_level=loglevel,
            disk_label=disk_label)
    